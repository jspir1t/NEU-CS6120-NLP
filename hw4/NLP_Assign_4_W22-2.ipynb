{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPN8xPAnd8V"
      },
      "source": [
        "# **CS 6120: Natural Language Processing - Prof. Ahmad Uzair** \n",
        "\n",
        "### Assignment 4: SVD, Cross-Language Word Embeddings and Pointwise Mutual Information\n",
        "\n",
        "### **Total points: 100**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJQ7jm3Wnd8e"
      },
      "source": [
        "# Q1. SVD (30 Points) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - A. Singular Value Decomposition (SVD) based distributed representation of text and documents. You can use python libraries for matrix decomposition (scipy). To demonstrate your work, use the example dataset (Table 2) of \"R. A. Harshman (1990). Indexing by latent semantic analysis. Journal of the American society for information science\". (10 Points)\n",
        "\n",
        " - B. Visualize (2-D) the documents and terms using library of your choice. (10 Points)\n",
        "\n",
        " - C. Implement a function that converts a query string to distributed representation and retrieves relevent documents. Visualize the the results as shown in Fig 1 of the paper. (10 Points)"
      ],
      "metadata": {
        "id": "PXSFfyAvqaMx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSGblHu-nd8r"
      },
      "source": [
        "# Q2. Cross-Language Word Embeddings (30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_Ll_UVnd8r"
      },
      "source": [
        "Different modeling choices for word embeddings may be ultimately evaluated by the effectiveness of classifiers, parsers, and other inference models that use those embeddings.<br>\n",
        "\n",
        "In this assignment, however, we will consider another common method of evaluating word embeddings: by judging the usefulness of pairwise distances between words in the embedding space.<br>\n",
        "\n",
        "Follow along with the examples in this notebook, and implement the sections of code flagged with TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yrEueudnd8t"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v3tjUX1nd8t"
      },
      "source": [
        "We'll start by downloading a plain-text version of the Shakespeare plays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhdaIOlUnd8t"
      },
      "outputs": [],
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/shakespeare_plays.txt\n",
        "lines = [s.split() for s in open('shakespeare_plays.txt')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxIxnZHend8t"
      },
      "source": [
        "Then, we'll estimate a simple word2vec model on the Shakespeare texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cil4QFfnd8u"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nIOk6wynd8v"
      },
      "source": [
        "Even with such a small training set size, you can perform some standard analogy tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B4E8xsUnd8v"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(positive=None, negative=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTQ-7LUXnd8v"
      },
      "source": [
        "For the rest of this assignment, we will focus on finding words with similar embeddings, both within and across languages. For example, what words are similar to the name of the title character of Othello?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVHUITVDnd8v"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(positive=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byZAitDBnd8w"
      },
      "source": [
        "This search uses cosine similarity. In the default API, you should see the same similarity between the words othello and desdemona as in the search results above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7NNUmodnd8x"
      },
      "outputs": [],
      "source": [
        "model.wv.similarity(None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm6KbU3ynd8x"
      },
      "source": [
        "TODO: Your first task, therefore, is to implement your own cosine similarity function so that you can reuse it outside of the context of the gensim model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oUxGtMmnd8x"
      },
      "outputs": [],
      "source": [
        "def cosim(v1, v2): \n",
        "    return None\n",
        "\n",
        "cosim(model.wv['othello'], model.wv['desdemona'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GupR8RQhnd8y"
      },
      "source": [
        "<h3>Evaluation: </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txhyTNxhnd8y"
      },
      "source": [
        "We could collect a lot of human judgments about how similar pairs of words, or pairs of Shakespearean characters, are. Then we could compare different word-embedding models by their ability to replicate these human judgments.<br>\n",
        "\n",
        "If we extend our ambition to multiple languages, however, we can use a word translation task to evaluate word embeddings.<br>\n",
        "\n",
        "We will use a subset of Facebook AI's FastText cross-language embeddings for several languages. Your task will be to compare English both to French, and to one more language from the following set: Arabic, German, Portuguese, Russian, Spanish, Vietnamese, and Chinese.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291BHZzKnd8y"
      },
      "outputs": [],
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.en.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.fr.vec\n",
        "\n",
        "# TODO: uncomment at least one of these to work with another language\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ar.vec\n",
        "#!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.de.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.pt.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ru.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.es.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.vi.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.zh.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC_bvhc5nd8z"
      },
      "source": [
        "We'll start by loading the word vectors from their textual file format to a dictionary mapping words to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJwRLxV4nd8z"
      },
      "outputs": [],
      "source": [
        "def vecref(s):\n",
        "    (word, srec) = s.split(' ', 1)\n",
        "    return (word, np.fromstring(srec, sep=' '))\n",
        "\n",
        "def ftvectors(fname):\n",
        "    return { k:v for (k, v) in [vecref(s) for s in open(fname)] if len(v) > 1} \n",
        "\n",
        "# loading vectors for english and french languages.\n",
        "envec = ftvectors('30k.en.vec')\n",
        "frvec = ftvectors('30k.fr.vec')\n",
        "\n",
        "# TODO: load vectors for one more language, such as zhvec (Chinese) just like english or french\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLv4xtend8z"
      },
      "outputs": [],
      "source": [
        "## TODO: implement this search function\n",
        "def mostSimilar(vec, vecDict):\n",
        "  ## Use your cosim function from above\n",
        "    mostSimilar = ''\n",
        "    similarity = 0\n",
        "    for row in vecDict.items():\n",
        "        csm = cosim(None, None)\n",
        "        if None:\n",
        "            similarity = None\n",
        "            mostSimilar = None\n",
        "    return (mostSimilar, similarity)\n",
        "\n",
        "## some example searches\n",
        "[mostSimilar(envec[e], frvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQvBpq9gnd80"
      },
      "source": [
        "TODO: Your next task is to write a simple function that takes a vector and a dictionary of vectors and finds the most similar item in the dictionary. For this assignment, a linear scan through the dictionary using your cosim function from above is acceptible.</br>\n",
        "\n",
        "Some matches make more sense than others. Note that computer most closely matches informatique, the French term for computer science. If you looked further down the list, you would see ordinateur, the term for computer. This is one weakness of a focus only on embeddings for word types independent of context.</br>\n",
        "\n",
        "To evalute cross-language embeddings more broadly, we'll look at a dataset of links between Wikipedia articles.</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juR293gand80"
      },
      "outputs": [],
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/links.tab\n",
        "links = [s.split() for s in open('links.tab')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slSuzF6Dnd81"
      },
      "outputs": [],
      "source": [
        "links[302]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suHKjqV_nd81"
      },
      "source": [
        "TODO: Evaluate the English and French embeddings by computing the proportion of English Wikipedia articles whose corresponding French article is also the closest word in embedding space. Skip English articles not covered by the word embedding dictionary. Since many articles, e.g., about named entities have the same title in English and French, compute the baseline accuracy achieved by simply echoing the English title as if it were French. Remember to iterate only over English Wikipedia articles, not the entire embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSgDagGpnd81"
      },
      "outputs": [],
      "source": [
        "## TODO: Compute English-French Wikipedia retrieval accuracy.\n",
        "t = 0\n",
        "b = 0\n",
        "g = 0\n",
        "for row in links:\n",
        "    if row[1] == 'fr':\n",
        "        if row[0] in envec.keys():\n",
        "            t += 1\n",
        "            if row[0] == row[2]:\n",
        "                b += 1\n",
        "            similar, _ = mostSimilar(None, None)\n",
        "            if None:\n",
        "                g += 1\n",
        "\n",
        "baselineAccuracy = b/t\n",
        "accuracy = g/t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xq-AO2fnd81"
      },
      "outputs": [],
      "source": [
        "print(baselineAccuracy, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmitxFend83"
      },
      "source": [
        "TODO: Compute accuracy and baseline (identity function) acccuracy for Englsih and another language besides French. Although the baseline will be lower for languages not written in the Roman alphabet (i.e., Arabic or Chinese), there are still many articles in those languages with headwords written in Roman characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rnIh9cBnd83"
      },
      "outputs": [],
      "source": [
        "## TODO: Compute English-X Wikipedia retrieval accuracy.\n",
        "#Follow the above procedure to do this task.\n",
        "\n",
        "print(baselineAccuracy, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaro4Ck2nd83"
      },
      "source": [
        "# Q 3. Mutual Information (40 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW564KoOnd83"
      },
      "source": [
        "Please read this paper https://aclanthology.org/J92-4003.pdf to answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIgA7vJnd84"
      },
      "source": [
        "**\"A quick fox jumps over the lazy dog. A quick fox jumps over the lazy dog. A quick fox jumps over the lazy dog. A quick fox jumps over the lazy dog. A quick fox jumps over the lazy dog. \"**\n",
        "\n",
        "1. Implement a function to compute the mutual information between pair of any adjacent words (w1,w2) of in the above text. (10 Points)\n",
        "\n",
        "2. What do you mean by semantic sticky pairs. Write a function that extracts semactic sticky pairs (10 Points)\n",
        "   \n",
        "   \n",
        "3. What is the relation between stickiness of the pair of words and mutual information ? (10 Points)\n",
        "   \n",
        "4. Explain the differences between Semantically sticky words and Adjacent sticky words ? (10 Points)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_Assign_4_W22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}