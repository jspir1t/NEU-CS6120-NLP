{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc97d6da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 1: Naive Bayes\n",
    "### Total Points: 100 points\n",
    "\n",
    "In this assignment, you would be working with SMS data that contains SPAM or HAM messages. When you take a look at your gmail account, you find that a few emails are classified as spam. Similarly, some text messages that are received on the phone are also classified as spam based on a set of characteristics such as wording and so on. \n",
    "\n",
    "Therefore, we are going to address this problem of detecting SPAM or HAM messages with the help of Naive Bayes algorithm.\n",
    "\n",
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03450ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584cc2",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "When reading the data, ensure that the '.csv' file is in the same location where your jupyter notebook is used. This way the files are organized and easy to read using the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9ffbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the data and removing columns that are not important. \n",
    "df = pd.read_csv('spam.csv', sep = ',', encoding = 'latin-1', usecols = lambda col: col not in [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fa8ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # print head of data frame with help of head function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecf75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming the columns so that we understand the columns easily.\n",
    "\n",
    "## Rename v1 as \"spam_or_ham\" and v2 as \"message\"\n",
    "\n",
    "df = df.rename(columns = {'v1': 'spam_or_ham', 'v2': 'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fc153c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam_or_ham</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam_or_ham                                            message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # print head of data frame with help of head function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749da04",
   "metadata": {},
   "source": [
    "## Count plot of the output categories: spam or ham\n",
    "\n",
    "Feel free to take a look at the output and whether the classes are balanced or imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c152e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = None)\n",
    "sns.countplot(df['spam_or_ham'])\n",
    "plt.title(\"Spam Vs. Ham Messages count\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8549f2",
   "metadata": {},
   "source": [
    "## Upsampling the minority class: (5 points)\n",
    "\n",
    "It is known that Naive bayes is not robust to class imbalance. It could be seen above that the data is quite imbalanced. Therefore, class balancing must be done before giving it to the Naive Bayes model for prediction. \n",
    "\n",
    "Feel free to use 'resample' library from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f27192",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hint: use resample from sklearn.utils\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df['spam_or_ham'] == 'ham']\n",
    "df_minority = df[df['spam_or_ham'] == 'spam']\n",
    "\n",
    "spam_upsample = resample(df_minority, replace = True, \n",
    "                        n_samples = df_majority.shape[0],\n",
    "                        random_state = 101)\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, spam_upsample])  # concat two data frames i,e majority class data set and upsampled minority class data set\n",
    "df_upsampled = df_upsampled.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9329bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4825, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just to ensure that upsampling was done successfully, take a look at the shape of the data in \n",
    "## this cell. \n",
    "\n",
    "# print the shape of data set with the help of shape function having \"spam\" as class label\n",
    "df_upsampled[df_upsampled['spam_or_ham'] == 'spam'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bf6e7",
   "metadata": {},
   "source": [
    "### Expected Output : \n",
    "(4825, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdea8155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4825, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ensure that the same number of data points are present for both 'spam' and 'ham' data\n",
    "\n",
    "# print the shape of data set with the help of shape function having \"ham\" as class label\n",
    "df_upsampled[df_upsampled['spam_or_ham'] == 'ham'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f01d5",
   "metadata": {},
   "source": [
    "### Expected Output : \n",
    "(4825, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61eb9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this cell, we are going to be dividing the data into train and test points\n",
    "## Ensure that you store the upsampled data in a variable called 'df_upsampled' \n",
    "## so that the below operations are performed successfully\n",
    "\n",
    "\n",
    "## Considering 3000 spam and 3000 ham data points\n",
    "spam_data_points_train = df_upsampled[df_upsampled['spam_or_ham'] == 'spam'].iloc[:3000]\n",
    "ham_data_points_train = df_upsampled[df_upsampled['spam_or_ham'] == 'ham'].iloc[:3000]\n",
    "\n",
    "## Considering the remaining data points for test\n",
    "spam_data_points_test = df_upsampled[df_upsampled['spam_or_ham'] == 'spam'].iloc[3000:]\n",
    "ham_data_points_test = df_upsampled[df_upsampled['spam_or_ham'] == 'ham'].iloc[3000:]\n",
    "\n",
    "## Concatenate the training ham and spam messages\n",
    "X_train = pd.concat([ham_data_points_train['message'], spam_data_points_train['message']])\n",
    "## Concatenating the training ham and spam outputs\n",
    "y_train = pd.concat([ham_data_points_train['spam_or_ham'], spam_data_points_train['spam_or_ham']])\n",
    "\n",
    "## Concatenating the test ham and spam messages\n",
    "X_test = pd.concat([ham_data_points_test['message'], spam_data_points_test['message']])\n",
    "## Concatenating the test ham and spam outputs\n",
    "y_test = pd.concat([ham_data_points_test['spam_or_ham'], spam_data_points_test['spam_or_ham']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6428047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3000\n",
       "spam    3000\n",
       "Name: spam_or_ham, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take a look at the total number of classes and their count using '.value_counts()' for y_train and y_test.\n",
    "## Ensure that there are equal number of spam and ham messages. \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe6517",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "spam    3000<br>\n",
    "ham     3000<br>\n",
    "Name: spam_or_ham, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2beae1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     1825\n",
       "spam    1825\n",
       "Name: spam_or_ham, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163f897",
   "metadata": {},
   "source": [
    "### Expected Output : \n",
    "spam    1825<br>\n",
    "ham     1825<br>\n",
    "Name: spam_or_ham, dtype: int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501699b",
   "metadata": {},
   "source": [
    "## Pre-process the messages: (15 points)\n",
    "\n",
    "We know that a message contains links, punctuation, stopwords and many other words that don't give a lot of meaning for the Naive Bayes model for prediction. \n",
    "\n",
    "In the cell below, one must implement text-preprocessing and remove links, punctuations and stopwords. It is also important to lowercase the letters so that 'Admire' and 'admire' are not treated as different words. \n",
    "\n",
    "In addition to this, perform stemming operation so that similar words are reduced. To know more about stemming, feel free to take a look at this link.\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd4ce1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "import string\n",
    "def clean_message(message):\n",
    "    '''\n",
    "    Input:\n",
    "        message: a string containing a message.\n",
    "    Output:\n",
    "        messages_cleaned: a list of words containing the processed message. \n",
    "\n",
    "    '''\n",
    "    message = re.sub(r'https?://\\S+', '', message)\n",
    "    message_punc_removed = message.translate(str.maketrans('', '', string.punctuation))\n",
    "    messages_tokenized = word_tokenize(message_punc_removed)\n",
    "    messages_stemmed = [PorterStemmer().stem(word) for word in messages_tokenized]\n",
    "    messages_cleaned = [word for word in messages_stemmed if word not in stopwords.words('english')]\n",
    "\n",
    "    return messages_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7632fe5",
   "metadata": {},
   "source": [
    "## Implement a find_occurrence function (5 points):\n",
    "\n",
    "In this function, we find the total occurrence of a word giving information such as label, word and frequency dictionary.\n",
    "\n",
    "Note that this function is used later in the code when we are going to be predicting the output using Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb282b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def find_occurrence(frequency, word, label):\n",
    "    '''\n",
    "    Params:\n",
    "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Return:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = frequency.get((word, label), 0)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2249d",
   "metadata": {},
   "source": [
    "## Converting output to numerical format:\n",
    "\n",
    "We have outputs as 'ham' or 'spam'. In the cell below, we convert it to a numerical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcdc2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With the use of mapping function, we replace\n",
    "## the label in the form of string to an integer. \n",
    "\n",
    "output_map = {'ham': 0, 'spam': 1}\n",
    "y_train = y_train.map(output_map)\n",
    "y_test = y_test.map(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dde0bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3000\n",
       "1    3000\n",
       "Name: spam_or_ham, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ensuring that there are equal number of classes on the training data. \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2959b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jos ask if u wana meet up?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Choosing a random message and taking a look at it.\n",
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e43c9",
   "metadata": {},
   "source": [
    "From the above cell output, it could be seen that there are a lot of words that don't add a lot of meaning to the text. \n",
    "\n",
    "Therefore, those words would be removed. It also reduces the computation time. \n",
    "\n",
    "Therefore, it is a good practice we are following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad3937ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jo', 'ask', 'u', 'wana', 'meet']\n"
     ]
    }
   ],
   "source": [
    "custom_message = X_train.iloc[0]\n",
    "\n",
    "# print cleaned message\n",
    "print(clean_message(custom_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cc440",
   "metadata": {},
   "source": [
    "We now use this function to pre-process the message and remove words that don't add a lot of meaning in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a762960",
   "metadata": {},
   "source": [
    "## Implementing message counter function: (10 points)\n",
    "\n",
    "It is now time to implement the count function for the messages. \n",
    "\n",
    "In this function, we count the occurrence of words and get the probabilities \n",
    "for the words based on the training data. \n",
    "\n",
    "In other words, we get the probability of occurrence of a word, given that the output is 'spam'.\n",
    "\n",
    "Similarly, we also compute the probability of occurence of a word, given that the output is 'ham'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5de61f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def message_counter(output_occurrence, messages, spam_or_ham):\n",
    "    '''\n",
    "    Params:\n",
    "        output_occurrence: a dictionary that will be used to map each pair to its frequency\n",
    "        messages: a list of messages\n",
    "        spam_or_ham: a list corresponding to the sentiment of each message (either 0 or 1)\n",
    "    Return:\n",
    "        output: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ## Steps :\n",
    "    # define the key, which is the word and label tuple\n",
    "    # if the key exists in the dictionary, increment the count\n",
    "    # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "    \n",
    "    for label, message in zip(spam_or_ham, messages):\n",
    "        for word in clean_message(message):\n",
    "            \n",
    "            output_occurrence[(word, label)] = find_occurrence(output_occurrence, word, label) + 1\n",
    "\n",
    "    return output_occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18238223",
   "metadata": {},
   "source": [
    "## Test your function with example messages:\n",
    "\n",
    "Feel free to run the cell below and understand whether the above function that you have defined is producing the optimum results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a4c58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('get', 1): 2,\n",
       " ('offer', 1): 1,\n",
       " ('upto', 1): 1,\n",
       " ('20', 1): 1,\n",
       " ('come', 0): 1,\n",
       " ('click', 1): 1,\n",
       " ('link', 1): 1,\n",
       " ('latest', 1): 1,\n",
       " ('car', 1): 1,\n",
       " ('canva', 0): 1,\n",
       " ('class', 0): 1,\n",
       " ('schedul', 0): 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "messages = ['get offer upto 20%', 'I am coming now', 'Click on the link', 'get a latest car', 'canvas class scheduled']\n",
    "ys = [1, 0, 1, 1, 0]\n",
    "message_counter(result,messages, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f89bb",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "{('get', 1): 2, <br>\n",
    " ('offer', 1): 1, <br>\n",
    " ('upto', 1): 1, <br>\n",
    " ('20', 1): 1, <br>\n",
    " ('i', 0): 1, <br>\n",
    " ('come', 0): 1, <br>\n",
    " ('click', 1): 1, <br>\n",
    " ('link', 1): 1, <br>\n",
    " ('latest', 1): 1, <br>\n",
    " ('car', 1): 1, <br>\n",
    " ('canva', 0): 1, <br>\n",
    " ('class', 0): 1, <br>\n",
    " ('schedul', 0): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bc62e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = message_counter({}, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eddf420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('jo', 0): 3,\n",
       " ('ask', 0): 76,\n",
       " ('u', 0): 612,\n",
       " ('wana', 0): 9,\n",
       " ('meet', 0): 69,\n",
       " ('im', 0): 277,\n",
       " ('free', 0): 39,\n",
       " ('moment', 0): 7,\n",
       " ('dayha', 0): 1,\n",
       " ('valuemorn', 0): 1,\n",
       " ('bring', 0): 25,\n",
       " ('hopeafternoon', 0): 1,\n",
       " ('faitheven', 0): 1,\n",
       " ('luvnight', 0): 1,\n",
       " ('restwish', 0): 1,\n",
       " ('find', 0): 32,\n",
       " ('todaygood', 0): 1,\n",
       " ('morn', 0): 48,\n",
       " ('ye', 0): 46,\n",
       " ('start', 0): 29,\n",
       " ('send', 0): 73,\n",
       " ('request', 0): 5,\n",
       " ('make', 0): 81,\n",
       " ('pain', 0): 21,\n",
       " ('came', 0): 18,\n",
       " ('back', 0): 77,\n",
       " ('bed', 0): 20,\n",
       " ('doubl', 0): 2,\n",
       " ('coin', 0): 5,\n",
       " ('factori', 0): 1,\n",
       " ('got', 0): 165,\n",
       " ('ta', 0): 10,\n",
       " ('cash', 0): 6,\n",
       " ('nitro', 0): 1,\n",
       " ('night', 0): 70,\n",
       " ('ha', 0): 69,\n",
       " ('end', 0): 20,\n",
       " ('anoth', 0): 24,\n",
       " ('day', 0): 116,\n",
       " ('come', 0): 198,\n",
       " ('special', 0): 25,\n",
       " ('way', 0): 74,\n",
       " ('may', 0): 26,\n",
       " ('smile', 0): 51,\n",
       " ('like', 0): 143,\n",
       " ('sunni', 0): 6,\n",
       " ('ray', 0): 5,\n",
       " ('leav', 0): 58,\n",
       " ('worri', 0): 28,\n",
       " ('blue', 0): 11,\n",
       " ('bay', 0): 4,\n",
       " ('gud', 0): 32,\n",
       " ('mrng', 0): 8,\n",
       " ('havent', 0): 26,\n",
       " ('plan', 0): 41,\n",
       " ('buy', 0): 39,\n",
       " ('later', 0): 78,\n",
       " ('check', 0): 29,\n",
       " ('alreadi', 0): 56,\n",
       " ('lido', 0): 3,\n",
       " ('onli', 0): 72,\n",
       " ('530', 0): 5,\n",
       " ('show', 0): 18,\n",
       " ('e', 0): 43,\n",
       " ('afternoon', 0): 18,\n",
       " ('finish', 0): 40,\n",
       " ('work', 0): 81,\n",
       " ('lunch', 0): 31,\n",
       " ('wake', 0): 15,\n",
       " ('hi', 0): 105,\n",
       " ('kindli', 0): 2,\n",
       " ('give', 0): 68,\n",
       " ('us', 0): 31,\n",
       " ('document', 0): 1,\n",
       " ('submit', 0): 2,\n",
       " ('loan', 0): 4,\n",
       " ('stapati', 0): 1,\n",
       " ('bookedth', 0): 1,\n",
       " ('hut', 0): 1,\n",
       " ('also', 0): 38,\n",
       " ('time', 0): 154,\n",
       " ('hey', 0): 77,\n",
       " ('next', 0): 30,\n",
       " ('sun', 0): 12,\n",
       " ('1030', 0): 2,\n",
       " ('basic', 0): 8,\n",
       " ('yoga', 0): 5,\n",
       " ('cours', 0): 10,\n",
       " ('bugi', 0): 5,\n",
       " ('go', 0): 263,\n",
       " ('pilat', 0): 4,\n",
       " ('intro', 0): 3,\n",
       " ('sat', 0): 17,\n",
       " ('tell', 0): 93,\n",
       " ('r', 0): 78,\n",
       " ('oki', 0): 7,\n",
       " ('thanx', 0): 17,\n",
       " ('moji', 0): 3,\n",
       " ('love', 0): 150,\n",
       " ('word', 0): 24,\n",
       " ('rich', 0): 2,\n",
       " ('think', 0): 82,\n",
       " ('might', 0): 24,\n",
       " ('miss', 0): 72,\n",
       " ('teach', 0): 6,\n",
       " ('til', 0): 16,\n",
       " ('twelv', 0): 2,\n",
       " ('lectur', 0): 4,\n",
       " ('two', 0): 23,\n",
       " ('damn', 0): 4,\n",
       " ('thi', 0): 147,\n",
       " ('thing', 0): 66,\n",
       " ('need', 0): 115,\n",
       " ('get', 0): 236,\n",
       " ('pleas', 0): 48,\n",
       " ('dont', 0): 178,\n",
       " ('say', 0): 76,\n",
       " ('helloooo', 0): 1,\n",
       " ('sweet', 0): 23,\n",
       " ('welcom', 0): 4,\n",
       " ('enjoy', 0): 20,\n",
       " ('full', 0): 12,\n",
       " ('joy', 0): 5,\n",
       " ('weather', 0): 4,\n",
       " ('mate', 0): 7,\n",
       " ('honeydid', 0): 1,\n",
       " ('ave', 0): 3,\n",
       " ('good', 0): 140,\n",
       " ('holiday', 0): 6,\n",
       " ('gimmi', 0): 1,\n",
       " ('de', 0): 17,\n",
       " ('gossx', 0): 1,\n",
       " ('cant', 0): 80,\n",
       " ('pick', 0): 52,\n",
       " ('phone', 0): 51,\n",
       " ('right', 0): 58,\n",
       " ('pl', 0): 56,\n",
       " ('messag', 0): 35,\n",
       " ('prashanthettan', 0): 1,\n",
       " ('mother', 0): 3,\n",
       " ('pass', 0): 6,\n",
       " ('away', 0): 13,\n",
       " ('last', 0): 45,\n",
       " ('pray', 0): 6,\n",
       " ('famili', 0): 13,\n",
       " ('hank', 0): 1,\n",
       " ('lotsli', 0): 1,\n",
       " ('ok', 0): 164,\n",
       " ('wif', 0): 13,\n",
       " ('co', 0): 53,\n",
       " ('2', 0): 202,\n",
       " ('tri', 0): 51,\n",
       " ('new', 0): 44,\n",
       " ('scare', 0): 3,\n",
       " ('dun', 0): 30,\n",
       " ('mah', 0): 11,\n",
       " ('said', 0): 60,\n",
       " ('loud', 0): 3,\n",
       " ('exam', 0): 12,\n",
       " ('march', 0): 8,\n",
       " ('ive', 0): 52,\n",
       " ('done', 0): 23,\n",
       " ('revis', 0): 1,\n",
       " ('fran', 0): 2,\n",
       " ('still', 0): 86,\n",
       " ('boyf', 0): 1,\n",
       " ('interviw', 0): 1,\n",
       " ('4', 0): 95,\n",
       " ('exet', 0): 2,\n",
       " ('bit', 0): 33,\n",
       " ('worriedx', 0): 1,\n",
       " ('hitechn', 0): 1,\n",
       " ('supportprovid', 0): 1,\n",
       " ('assist', 0): 1,\n",
       " ('custom', 0): 8,\n",
       " ('call', 0): 189,\n",
       " ('email', 0): 9,\n",
       " ('someon', 0): 32,\n",
       " ('smoke', 0): 15,\n",
       " ('everi', 0): 21,\n",
       " ('week', 0): 52,\n",
       " ('becaus', 0): 24,\n",
       " ('text', 0): 56,\n",
       " ('want', 0): 129,\n",
       " ('take', 0): 88,\n",
       " ('didnt', 0): 51,\n",
       " ('callon', 0): 1,\n",
       " ('friday', 0): 13,\n",
       " ('assum', 0): 3,\n",
       " ('wont', 0): 30,\n",
       " ('year', 0): 46,\n",
       " ('couldnt', 0): 3,\n",
       " ('wors', 0): 6,\n",
       " ('andro', 0): 2,\n",
       " ('ice', 0): 5,\n",
       " ('etc', 0): 4,\n",
       " ('doc', 0): 5,\n",
       " ('nice', 0): 34,\n",
       " ('shirt', 0): 5,\n",
       " ('hubbi', 0): 1,\n",
       " ('fite', 0): 1,\n",
       " ('one', 0): 108,\n",
       " ('budget', 0): 3,\n",
       " ('ltgt', 0): 177,\n",
       " ('k', 0): 45,\n",
       " ('help', 0): 22,\n",
       " ('load', 0): 9,\n",
       " ('card', 0): 9,\n",
       " ('abi', 0): 2,\n",
       " ('hwkeep', 0): 1,\n",
       " ('post', 0): 11,\n",
       " ('luv', 0): 15,\n",
       " ('mj', 0): 1,\n",
       " ('dearshal', 0): 1,\n",
       " ('mail', 0): 18,\n",
       " ('tonitebusi', 0): 1,\n",
       " ('streetshal', 0): 1,\n",
       " ('updat', 0): 2,\n",
       " ('toniteth', 0): 1,\n",
       " ('look', 0): 30,\n",
       " ('okvarunnathu', 0): 1,\n",
       " ('edukkukaye', 0): 1,\n",
       " ('raksha', 0): 1,\n",
       " ('ollubut', 0): 1,\n",
       " ('real', 0): 17,\n",
       " ('sens', 0): 5,\n",
       " ('tomarrow', 0): 1,\n",
       " ('court', 0): 1,\n",
       " ('ltdecimalgt', 0): 12,\n",
       " ('bu', 0): 20,\n",
       " ('stand', 0): 6,\n",
       " ('9', 0): 14,\n",
       " ('wn', 0): 2,\n",
       " ('hurt', 0): 24,\n",
       " ('prsn', 0): 1,\n",
       " ('close', 0): 13,\n",
       " ('fight', 0): 9,\n",
       " ('wit', 0): 15,\n",
       " ('dem', 0): 3,\n",
       " ('coz', 0): 9,\n",
       " ('somtim', 0): 1,\n",
       " ('di', 0): 15,\n",
       " ('save', 0): 8,\n",
       " ('relat', 0): 3,\n",
       " ('bt', 0): 12,\n",
       " ('quiet', 0): 1,\n",
       " ('nothin', 0): 2,\n",
       " ('eveb', 0): 2,\n",
       " ('home', 0): 109,\n",
       " ('lovin', 0): 1,\n",
       " ('pretti', 0): 10,\n",
       " ('long', 0): 23,\n",
       " ('hair', 0): 19,\n",
       " ('wat', 0): 67,\n",
       " ('thk', 0): 35,\n",
       " ('cut', 0): 10,\n",
       " ('quit', 0): 27,\n",
       " ('short', 0): 5,\n",
       " ('leh', 0): 21,\n",
       " ('minimum', 0): 1,\n",
       " ('walk', 0): 21,\n",
       " ('3mile', 0): 1,\n",
       " ('sure', 0): 47,\n",
       " ('ill', 0): 154,\n",
       " ('see', 0): 93,\n",
       " ('1', 0): 27,\n",
       " ('number', 0): 38,\n",
       " ('gon', 0): 38,\n",
       " ('na', 0): 52,\n",
       " ('massiv', 0): 2,\n",
       " ('ass', 0): 7,\n",
       " ('id', 0): 21,\n",
       " ('rather', 0): 5,\n",
       " ('involv', 0): 2,\n",
       " ('possibl', 0): 6,\n",
       " ('joke', 0): 12,\n",
       " ('girl', 0): 23,\n",
       " ('situat', 0): 8,\n",
       " ('seeker', 0): 1,\n",
       " ('otherwis', 0): 5,\n",
       " ('part', 0): 11,\n",
       " ('job', 0): 28,\n",
       " ('natuit', 0): 2,\n",
       " ('doe', 0): 15,\n",
       " ('uncl', 0): 10,\n",
       " ('timi', 0): 1,\n",
       " ('clear', 0): 2,\n",
       " ('car', 0): 29,\n",
       " ('youll', 0): 12,\n",
       " ('never', 0): 23,\n",
       " ('believ', 0): 12,\n",
       " ('actual', 0): 17,\n",
       " ('taunton', 0): 3,\n",
       " ('wow', 0): 6,\n",
       " ('sleep', 0): 46,\n",
       " ('nt', 0): 7,\n",
       " ('feel', 0): 60,\n",
       " ('well', 0): 80,\n",
       " ('absolut', 0): 1,\n",
       " ('south', 0): 4,\n",
       " ('park', 0): 10,\n",
       " ('recent', 0): 3,\n",
       " ('watch', 0): 40,\n",
       " ('offic', 0): 14,\n",
       " ('knw', 0): 13,\n",
       " ('ur', 0): 152,\n",
       " ('frnd', 0): 16,\n",
       " ('classmat', 0): 1,\n",
       " ('seventeen', 0): 1,\n",
       " ('pound', 0): 3,\n",
       " ('seven', 0): 3,\n",
       " ('hundr', 0): 2,\n",
       " ('ml', 0): 1,\n",
       " ('\\x89ûò', 0): 8,\n",
       " ('hope', 0): 66,\n",
       " ('howz', 0): 4,\n",
       " ('person', 0): 34,\n",
       " ('stori', 0): 14,\n",
       " ('would', 0): 51,\n",
       " ('b', 0): 38,\n",
       " ('wot', 0): 14,\n",
       " ('drinkin', 0): 2,\n",
       " ('dancin', 0): 1,\n",
       " ('eatin', 0): 2,\n",
       " ('cinema', 0): 5,\n",
       " ('ard', 0): 13,\n",
       " ('6', 0): 15,\n",
       " ('dat', 0): 26,\n",
       " ('lor', 0): 102,\n",
       " ('telugu', 0): 1,\n",
       " ('moviewat', 0): 1,\n",
       " ('abt', 0): 19,\n",
       " ('realli', 0): 61,\n",
       " ('shit', 0): 23,\n",
       " ('befor', 0): 37,\n",
       " ('tomorrow', 0): 54,\n",
       " ('know', 0): 148,\n",
       " ('awak', 0): 4,\n",
       " ('durban', 0): 2,\n",
       " ('though', 0): 16,\n",
       " ('shd', 0): 4,\n",
       " ('n', 0): 78,\n",
       " ('fun', 0): 15,\n",
       " ('bar', 0): 4,\n",
       " ('town', 0): 19,\n",
       " ('someth', 0): 44,\n",
       " ('sound', 0): 18,\n",
       " ('msg', 0): 38,\n",
       " ('meim', 0): 1,\n",
       " ('ad', 0): 8,\n",
       " ('crap', 0): 1,\n",
       " ('nite', 0): 10,\n",
       " ('wa', 0): 139,\n",
       " ('borin', 0): 1,\n",
       " ('without', 0): 22,\n",
       " ('ya', 0): 36,\n",
       " ('boggi', 0): 1,\n",
       " ('bore', 0): 17,\n",
       " ('biatch', 0): 1,\n",
       " ('wait', 0): 61,\n",
       " ('nxt', 0): 5,\n",
       " ('il', 0): 4,\n",
       " ('7', 0): 16,\n",
       " ('ex', 0): 7,\n",
       " ('better', 0): 26,\n",
       " ('eat', 0): 34,\n",
       " ('smth', 0): 9,\n",
       " ('els', 0): 15,\n",
       " ('guilti', 0): 2,\n",
       " ('old', 0): 11,\n",
       " ('airport', 0): 3,\n",
       " ('road', 0): 6,\n",
       " ('630', 0): 2,\n",
       " ('oredi', 0): 10,\n",
       " ('lot', 0): 35,\n",
       " ('pple', 0): 2,\n",
       " ('ani', 0): 68,\n",
       " ('gift', 0): 9,\n",
       " ('anyth', 0): 50,\n",
       " ('bad', 0): 18,\n",
       " ('ìï', 0): 27,\n",
       " ('gf', 0): 1,\n",
       " ('oop', 0): 3,\n",
       " ('die', 0): 10,\n",
       " ('even', 0): 56,\n",
       " ('yeah', 0): 57,\n",
       " ('horribl', 0): 3,\n",
       " ('gal', 0): 11,\n",
       " ('knew', 0): 10,\n",
       " ('yest', 0): 6,\n",
       " ('idc', 0): 1,\n",
       " ('weasel', 0): 2,\n",
       " ('twice', 0): 5,\n",
       " ('row', 0): 4,\n",
       " ('result', 0): 5,\n",
       " ('hear', 0): 21,\n",
       " ('scream', 0): 8,\n",
       " ('minut', 0): 32,\n",
       " ('caus', 0): 17,\n",
       " ('gyno', 0): 1,\n",
       " ('shove', 0): 2,\n",
       " ('belong', 0): 1,\n",
       " ('machan', 0): 2,\n",
       " ('gym', 0): 6,\n",
       " ('wil', 0): 10,\n",
       " ('late', 0): 38,\n",
       " ('goodnight', 0): 4,\n",
       " ('dice', 0): 1,\n",
       " ('art', 0): 4,\n",
       " ('class', 0): 28,\n",
       " ('thru', 0): 4,\n",
       " ('thank', 0): 51,\n",
       " ('idea', 0): 5,\n",
       " ('tot', 0): 13,\n",
       " ('outsid', 0): 11,\n",
       " ('darren', 0): 5,\n",
       " ('shop', 0): 22,\n",
       " ('ju', 0): 18,\n",
       " ('went', 0): 42,\n",
       " ('sim', 0): 5,\n",
       " ('lim', 0): 1,\n",
       " ('mp3', 0): 1,\n",
       " ('player', 0): 7,\n",
       " ('sfirst', 0): 1,\n",
       " ('timedhoni', 0): 1,\n",
       " ('rock', 0): 9,\n",
       " ('sea', 0): 8,\n",
       " ('lay', 0): 2,\n",
       " ('envelop', 0): 4,\n",
       " ('paper', 0): 10,\n",
       " ('3', 0): 26,\n",
       " ('singl', 0): 3,\n",
       " ('line', 0): 6,\n",
       " ('big', 0): 25,\n",
       " ('mean', 0): 29,\n",
       " ('best', 0): 20,\n",
       " ('life', 0): 49,\n",
       " ('fine', 0): 26,\n",
       " ('bbdpooja', 0): 1,\n",
       " ('pimpleseven', 0): 1,\n",
       " ('becom', 0): 7,\n",
       " ('blackand', 0): 1,\n",
       " ('rite', 0): 11,\n",
       " ('cold', 0): 3,\n",
       " ('wear', 0): 4,\n",
       " ('sweatter', 0): 1,\n",
       " ('celebr', 0): 6,\n",
       " ('båõday', 0): 2,\n",
       " ('engag', 0): 1,\n",
       " ('fixd', 0): 1,\n",
       " ('th', 0): 8,\n",
       " ('month', 0): 21,\n",
       " ('shock', 0): 3,\n",
       " ('bthmm', 0): 1,\n",
       " ('njan', 0): 1,\n",
       " ('vilikkamt', 0): 1,\n",
       " ('ws', 0): 3,\n",
       " ('al', 0): 6,\n",
       " ('sudn', 0): 1,\n",
       " ('hmm', 0): 8,\n",
       " ('anyway', 0): 19,\n",
       " ('holla', 0): 3,\n",
       " ('whenev', 0): 9,\n",
       " ('around', 0): 33,\n",
       " ('excus', 0): 6,\n",
       " ('creep', 0): 1,\n",
       " ('peopl', 0): 32,\n",
       " ('sarasota', 0): 2,\n",
       " ('broke', 0): 4,\n",
       " ('list', 0): 6,\n",
       " ('reason', 0): 14,\n",
       " ('whi', 0): 43,\n",
       " ('nobodi', 0): 5,\n",
       " ('sarcast', 0): 1,\n",
       " ('faggi', 0): 2,\n",
       " ('rememb', 0): 16,\n",
       " ('dobbi', 0): 2,\n",
       " ('bowl', 0): 1,\n",
       " ('youv', 0): 5,\n",
       " ('sorri', 0): 97,\n",
       " ('tabl', 0): 3,\n",
       " ('book', 0): 18,\n",
       " ('half', 0): 21,\n",
       " ('eight', 0): 2,\n",
       " ('let', 0): 42,\n",
       " ('fyi', 0): 3,\n",
       " ('usf', 0): 8,\n",
       " ('swing', 0): 9,\n",
       " ('room', 0): 20,\n",
       " ('messageno', 0): 2,\n",
       " ('responcewhat', 0): 2,\n",
       " ('happend', 0): 3,\n",
       " ('aftr', 0): 4,\n",
       " ('clean', 0): 4,\n",
       " ('hous', 0): 26,\n",
       " ('lul', 0): 1,\n",
       " ('gettin', 0): 8,\n",
       " ('juici', 0): 2,\n",
       " ('gossip', 0): 1,\n",
       " ('hospit', 0): 9,\n",
       " ('nurs', 0): 1,\n",
       " ('talk', 0): 39,\n",
       " ('fat', 0): 7,\n",
       " ('obes', 0): 1,\n",
       " ('oyea', 0): 1,\n",
       " ('laugh', 0): 13,\n",
       " ('spontan', 0): 1,\n",
       " ('care', 0): 49,\n",
       " ('probabl', 0): 20,\n",
       " ('dear', 0): 62,\n",
       " ('amp', 0): 57,\n",
       " ('friend', 0): 51,\n",
       " ('goodeven', 0): 1,\n",
       " ('insid', 0): 6,\n",
       " ('officestil', 0): 1,\n",
       " ('fill', 0): 7,\n",
       " ('formsdon', 0): 1,\n",
       " ('bognor', 0): 1,\n",
       " ('splendid', 0): 1,\n",
       " ('much', 0): 70,\n",
       " ('textin', 0): 1,\n",
       " ('bout', 0): 7,\n",
       " ('beauti', 0): 15,\n",
       " ('truth', 0): 9,\n",
       " ('graviti', 0): 4,\n",
       " ('read', 0): 17,\n",
       " ('heart', 0): 29,\n",
       " ('light', 0): 9,\n",
       " ('veri', 0): 49,\n",
       " ('heavi', 0): 7,\n",
       " ('buzzzz', 0): 1,\n",
       " ('grin', 0): 8,\n",
       " ('buzz', 0): 6,\n",
       " ('chest', 0): 1,\n",
       " ('cock', 0): 2,\n",
       " ('keep', 0): 40,\n",
       " ('vibrat', 0): 2,\n",
       " ('shake', 0): 2,\n",
       " ('rat', 0): 1,\n",
       " ('ever', 0): 24,\n",
       " ('vote', 0): 4,\n",
       " ('theme', 0): 1,\n",
       " ('theyll', 0): 1,\n",
       " ('understand', 0): 11,\n",
       " ('wine', 0): 9,\n",
       " ('slurp', 0): 1,\n",
       " ('great', 0): 56,\n",
       " ('success', 0): 7,\n",
       " ('tau', 0): 1,\n",
       " ('sar', 0): 1,\n",
       " ('piah', 0): 1,\n",
       " ('haha', 0): 29,\n",
       " ('sec', 0): 3,\n",
       " ('intrepid', 0): 1,\n",
       " ('duo', 0): 1,\n",
       " ('soon', 0): 37,\n",
       " ('master', 0): 3,\n",
       " ('easier', 0): 3,\n",
       " ('kate', 0): 6,\n",
       " ('bloodi', 0): 3,\n",
       " ('babyjontet', 0): 1,\n",
       " ('txt', 0): 8,\n",
       " ('xxx', 0): 13,\n",
       " ('nope', 0): 10,\n",
       " ('onlin', 0): 15,\n",
       " ('ring', 0): 7,\n",
       " ('asap', 0): 4,\n",
       " ('oh', 0): 65,\n",
       " ('kkwhere', 0): 2,\n",
       " ('test', 0): 18,\n",
       " ('si', 0): 14,\n",
       " ('yet', 0): 29,\n",
       " ('disturb', 0): 5,\n",
       " ('liao', 0): 19,\n",
       " ('nauseou', 0): 1,\n",
       " ('piss', 0): 5,\n",
       " ('today', 0): 80,\n",
       " ('pig', 0): 4,\n",
       " ('diet', 0): 5,\n",
       " ('hungri', 0): 6,\n",
       " ('guess', 0): 26,\n",
       " ('busi', 0): 15,\n",
       " ('solv', 0): 3,\n",
       " ('case', 0): 8,\n",
       " ('man', 0): 28,\n",
       " ('found', 0): 8,\n",
       " ('murder', 0): 13,\n",
       " ('1hi', 0): 2,\n",
       " ('wife', 0): 15,\n",
       " ('polic', 0): 4,\n",
       " ('2polic', 0): 2,\n",
       " ('question', 0): 12,\n",
       " ('everyon', 0): 13,\n",
       " ('3wife', 0): 2,\n",
       " ('siri', 0): 2,\n",
       " ('took', 0): 13,\n",
       " ('place', 0): 42,\n",
       " ('4cook', 0): 2,\n",
       " ('cook', 0): 5,\n",
       " ('5garden', 0): 2,\n",
       " ('veget', 0): 2,\n",
       " ('6housemaid', 0): 2,\n",
       " ('7children', 0): 2,\n",
       " ('play', 0): 19,\n",
       " ('8neighbour', 0): 2,\n",
       " ('marriag', 0): 5,\n",
       " ('arrest', 0): 4,\n",
       " ('immedi', 0): 4,\n",
       " ('repli', 0): 28,\n",
       " ('brilliant', 0): 3,\n",
       " ('haf', 0): 11,\n",
       " ('stupid', 0): 5,\n",
       " ('da', 0): 85,\n",
       " ('v', 0): 28,\n",
       " ('cam', 0): 2,\n",
       " ('yan', 0): 3,\n",
       " ('jiu', 0): 3,\n",
       " ('vill', 0): 1,\n",
       " ('frm', 0): 8,\n",
       " ('10', 0): 6,\n",
       " ('den', 0): 14,\n",
       " ('hop', 0): 4,\n",
       " ('parco', 0): 2,\n",
       " ('nb', 0): 2,\n",
       " ('cine', 0): 3,\n",
       " ('orc', 0): 1,\n",
       " ('mrt', 0): 5,\n",
       " ('hip', 0): 1,\n",
       " ('ni8', 0): 6,\n",
       " ('dearslp', 0): 1,\n",
       " ('welltak', 0): 1,\n",
       " ('careswt', 0): 1,\n",
       " ('dreamsmuah', 0): 1,\n",
       " ('babe', 0): 51,\n",
       " ('hour', 0): 28,\n",
       " ('almost', 0): 7,\n",
       " ('internet', 0): 5,\n",
       " ('prospect', 0): 2,\n",
       " ('lazi', 0): 6,\n",
       " ('bleak', 0): 1,\n",
       " ('hmmm', 0): 7,\n",
       " ('happi', 0): 80,\n",
       " ('catch', 0): 7,\n",
       " ('sell', 0): 11,\n",
       " ('batteri', 0): 2,\n",
       " ('gram', 0): 4,\n",
       " ('sayi', 0): 1,\n",
       " ('fast', 0): 9,\n",
       " ('easi', 0): 12,\n",
       " ('money', 0): 29,\n",
       " ('noth', 0): 23,\n",
       " ('risk', 0): 2,\n",
       " ('sort', 0): 14,\n",
       " ('stuff', 0): 29,\n",
       " ('fuckin', 0): 2,\n",
       " ('showr', 0): 1,\n",
       " ('pub', 0): 12,\n",
       " ('wish', 0): 46,\n",
       " ('opportun', 0): 2,\n",
       " ('thought', 0): 31,\n",
       " ('kiss', 0): 16,\n",
       " ('pump', 0): 1,\n",
       " ('petrol', 0): 2,\n",
       " ('rain', 0): 10,\n",
       " ('thatnow', 0): 1,\n",
       " ('plu', 0): 11,\n",
       " ('stop', 0): 30,\n",
       " ('run', 0): 14,\n",
       " ('pa', 0): 21,\n",
       " ('abl', 0): 13,\n",
       " ('rdi', 0): 1,\n",
       " ('ship', 0): 5,\n",
       " ('comp', 0): 1,\n",
       " ('ruin', 0): 3,\n",
       " ('thesi', 0): 2,\n",
       " ('staffsciencenusedusgphyhcmkteachingpc1323', 0): 1,\n",
       " ('imin', 0): 1,\n",
       " ('towndontmatt', 0): 1,\n",
       " ('urgoin', 0): 1,\n",
       " ('outl8r', 0): 1,\n",
       " ('monday', 0): 6,\n",
       " ('drink', 0): 17,\n",
       " ('tap', 0): 2,\n",
       " ('spile', 0): 2,\n",
       " ('ga', 0): 9,\n",
       " ('st', 0): 7,\n",
       " ('broad', 0): 2,\n",
       " ('canal', 0): 2,\n",
       " ('opposit', 0): 1,\n",
       " ('side', 0): 7,\n",
       " ('drop', 0): 10,\n",
       " ('theyr', 0): 3,\n",
       " ('keen', 0): 1,\n",
       " ('kind', 0): 9,\n",
       " ('shouldnt', 0): 4,\n",
       " ('tomo', 0): 10,\n",
       " ('mind', 0): 26,\n",
       " ('b4', 0): 7,\n",
       " ('omw', 0): 4,\n",
       " ('castor', 0): 2,\n",
       " ('naughti', 0): 3,\n",
       " ('pix', 0): 4,\n",
       " ('txting', 0): 1,\n",
       " ('drive', 0): 30,\n",
       " ('use', 0): 29,\n",
       " ('soc', 0): 1,\n",
       " ('dunno', 0): 19,\n",
       " ('type', 0): 10,\n",
       " ('ar', 0): 4,\n",
       " ('yo', 0): 24,\n",
       " ('guy', 0): 33,\n",
       " ('figur', 0): 6,\n",
       " ('alcohol', 0): 2,\n",
       " ('jay', 0): 12,\n",
       " ('safe', 0): 8,\n",
       " ('spend', 0): 9,\n",
       " ('weed', 0): 6,\n",
       " ('accident', 0): 3,\n",
       " ('brought', 0): 1,\n",
       " ('em', 0): 7,\n",
       " ('box', 0): 3,\n",
       " ('lei', 0): 15,\n",
       " ('neva', 0): 10,\n",
       " ('imma', 0): 3,\n",
       " ('definit', 0): 5,\n",
       " ('restock', 0): 2,\n",
       " ('thanksgiv', 0): 2,\n",
       " ('dinner', 0): 19,\n",
       " ('ho', 0): 6,\n",
       " ('belli', 0): 3,\n",
       " ('guessin', 0): 1,\n",
       " ('aint', 0): 7,\n",
       " ('aight', 0): 25,\n",
       " ('tonight', 0): 35,\n",
       " ('li', 0): 1,\n",
       " ('hai', 0): 6,\n",
       " ('repeat', 0): 1,\n",
       " ('wast', 0): 7,\n",
       " ('breath', 0): 3,\n",
       " ('neck', 0): 2,\n",
       " ('bud', 0): 2,\n",
       " ('youd', 0): 4,\n",
       " ('track', 0): 5,\n",
       " ('carlosl', 0): 1,\n",
       " ('trust', 0): 6,\n",
       " ('geeeee', 0): 2,\n",
       " ('eh', 0): 7,\n",
       " ('answer', 0): 14,\n",
       " ('mayb', 0): 24,\n",
       " ('reboot', 0): 3,\n",
       " ('ym', 0): 4,\n",
       " ('photo', 0): 7,\n",
       " ('drugdeal', 0): 1,\n",
       " ('mm', 0): 5,\n",
       " ('railway', 0): 2,\n",
       " ('mani', 0): 34,\n",
       " ('lose', 0): 22,\n",
       " ('bcoz', 0): 8,\n",
       " ('brah', 0): 1,\n",
       " ('fact', 0): 5,\n",
       " ('although', 0): 1,\n",
       " ('armand', 0): 3,\n",
       " ('eventu', 0): 1,\n",
       " ('build', 0): 4,\n",
       " ('toler', 0): 1,\n",
       " ('consid', 0): 5,\n",
       " ('fuck', 0): 31,\n",
       " ('hit', 0): 8,\n",
       " ('def', 0): 3,\n",
       " ('readi', 0): 22,\n",
       " ('importantli', 0): 1,\n",
       " ('discuss', 0): 5,\n",
       " ('littl', 0): 15,\n",
       " ('difficult', 0): 7,\n",
       " ('simpl', 0): 7,\n",
       " ('enter', 0): 7,\n",
       " ('1appledayno', 0): 1,\n",
       " ('doctor', 0): 5,\n",
       " ('1tulsi', 0): 1,\n",
       " ('leafdayno', 0): 1,\n",
       " ('cancer', 0): 4,\n",
       " ('1lemondayno', 0): 1,\n",
       " ('1cup', 0): 1,\n",
       " ('milkdayno', 0): 1,\n",
       " ('bone', 0): 1,\n",
       " ('problm', 0): 1,\n",
       " ('litr', 0): 1,\n",
       " ('watrdayno', 0): 1,\n",
       " ('diseas', 0): 1,\n",
       " ('snd', 0): 1,\n",
       " ('saw', 0): 19,\n",
       " ('doll', 0): 1,\n",
       " ('patrick', 0): 1,\n",
       " ('swayz', 0): 1,\n",
       " ('hict', 0): 1,\n",
       " ('employe', 0): 1,\n",
       " ('sake', 0): 2,\n",
       " ('x', 0): 27,\n",
       " ('bruce', 0): 1,\n",
       " ('fletcher', 0): 1,\n",
       " ('kkim', 0): 2,\n",
       " ('tirunelvali', 0): 1,\n",
       " ('amount', 0): 3,\n",
       " ('dress', 0): 2,\n",
       " ('moneyi', 0): 1,\n",
       " ('feb', 0): 1,\n",
       " ('differ', 0): 11,\n",
       " ('versu', 0): 1,\n",
       " ('hr', 0): 8,\n",
       " ('juz', 0): 14,\n",
       " ('italian', 0): 4,\n",
       " ('slice', 0): 3,\n",
       " ('breadstick', 0): 1,\n",
       " ('lol', 0): 47,\n",
       " ('urgh', 0): 1,\n",
       " ('coach', 0): 1,\n",
       " ('hot', 0): 6,\n",
       " ('smell', 0): 2,\n",
       " ('chip', 0): 1,\n",
       " ('especi', 0): 7,\n",
       " ('duvet', 0): 1,\n",
       " ('predict', 0): 3,\n",
       " ('jog', 0): 1,\n",
       " ('welp', 0): 3,\n",
       " ('semiobscur', 0): 1,\n",
       " ('cheer', 0): 9,\n",
       " ('first', 0): 39,\n",
       " ('stay', 0): 17,\n",
       " ('tough', 0): 4,\n",
       " ('optimist', 0): 1,\n",
       " ('improv', 0): 1,\n",
       " ('train', 0): 14,\n",
       " ('northampton', 0): 1,\n",
       " ('afraid', 0): 2,\n",
       " ('low', 0): 2,\n",
       " ('happen', 0): 26,\n",
       " ('date', 0): 9,\n",
       " ('tortur', 0): 1,\n",
       " ('england', 0): 1,\n",
       " ('per', 0): 8,\n",
       " ('mell', 0): 2,\n",
       " ('oru', 0): 2,\n",
       " ('minnaminungint', 0): 1,\n",
       " ('nurungu', 0): 1,\n",
       " ('vettam', 0): 1,\n",
       " ('set', 0): 14,\n",
       " ('callertun', 0): 6,\n",
       " ('caller', 0): 4,\n",
       " ('press', 0): 4,\n",
       " ('copi', 0): 5,\n",
       " ('hen', 0): 1,\n",
       " ('awesom', 0): 14,\n",
       " ('lem', 0): 4,\n",
       " ('beach', 0): 1,\n",
       " ('expect', 0): 5,\n",
       " ('doesnt', 0): 15,\n",
       " ('vomit', 0): 5,\n",
       " ('temp', 0): 3,\n",
       " ('unmit', 0): 1,\n",
       " ('howev', 0): 3,\n",
       " ('live', 0): 18,\n",
       " ('min', 0): 23,\n",
       " ('rhythm', 0): 2,\n",
       " ('establish', 0): 1,\n",
       " ('bodi', 0): 5,\n",
       " ('learn', 0): 5,\n",
       " ('geniu', 0): 2,\n",
       " ('brother', 0): 13,\n",
       " ('skype', 0): 4,\n",
       " ('congrat', 0): 6,\n",
       " ('kanowhr', 0): 1,\n",
       " ('treat', 0): 14,\n",
       " ('maga', 0): 2,\n",
       " ('asleep', 0): 2,\n",
       " ('j', 0): 3,\n",
       " ('could', 0): 39,\n",
       " ('blake', 0): 2,\n",
       " ('address', 0): 11,\n",
       " ('occur', 0): 2,\n",
       " ('sleepwellamptak', 0): 2,\n",
       " ('aiyah', 0): 3,\n",
       " ('drizzl', 0): 1,\n",
       " ('least', 0): 7,\n",
       " ('made', 0): 20,\n",
       " ('payment', 0): 1,\n",
       " ('fedex', 0): 1,\n",
       " ('jess', 0): 2,\n",
       " ('eve', 0): 5,\n",
       " ('turn', 0): 9,\n",
       " ('randomlli', 0): 1,\n",
       " ('within', 0): 5,\n",
       " ('5min', 0): 2,\n",
       " ('open', 0): 12,\n",
       " ('ladi', 0): 4,\n",
       " ('genu', 0): 1,\n",
       " ('second', 0): 15,\n",
       " ('valentin', 0): 10,\n",
       " ('game', 0): 8,\n",
       " ('5', 0): 21,\n",
       " ('que', 0): 3,\n",
       " ('colour', 0): 6,\n",
       " ('suit', 0): 4,\n",
       " ('bestrpli', 0): 1,\n",
       " ('feelin', 0): 1,\n",
       " ('pete', 0): 4,\n",
       " ('wuld', 0): 1,\n",
       " ('nuther', 0): 2,\n",
       " ('roommat', 0): 3,\n",
       " ('forev', 0): 5,\n",
       " ('ah', 0): 16,\n",
       " ('confus', 0): 4,\n",
       " ('wrong', 0): 7,\n",
       " ('invit', 0): 11,\n",
       " ('tho', 0): 9,\n",
       " ('mine', 0): 13,\n",
       " ('lookin', 0): 2,\n",
       " ('er', 0): 5,\n",
       " ('hello', 0): 32,\n",
       " ('didn\\x89û÷t', 0): 2,\n",
       " ('limp', 0): 1,\n",
       " ('slowli', 0): 10,\n",
       " ('follow', 0): 3,\n",
       " ('aa', 0): 1,\n",
       " ('exhaust', 0): 4,\n",
       " ('hang', 0): 5,\n",
       " ('sent', 0): 30,\n",
       " ('lanr', 0): 1,\n",
       " ('fakey', 0): 1,\n",
       " ('eckankar', 0): 1,\n",
       " ('detail', 0): 10,\n",
       " ('told', 0): 32,\n",
       " ('chennaibecaus', 0): 1,\n",
       " ('aft', 0): 12,\n",
       " ('bath', 0): 16,\n",
       " ('dog', 0): 3,\n",
       " ('diff', 0): 2,\n",
       " ('pop', 0): 4,\n",
       " ('ibuprofen', 0): 1,\n",
       " ('whatev', 0): 11,\n",
       " ('yup', 0): 22,\n",
       " ('ssindia', 0): 1,\n",
       " ('draw', 0): 4,\n",
       " ('seri', 0): 3,\n",
       " ('african', 0): 1,\n",
       " ('soil', 0): 1,\n",
       " ('hide', 0): 3,\n",
       " ('thousand', 0): 1,\n",
       " ('secret', 0): 4,\n",
       " ('wonder', 0): 25,\n",
       " ('n8', 0): 1,\n",
       " ('ìll', 0): 2,\n",
       " ('project', 0): 10,\n",
       " ('tmr', 0): 19,\n",
       " ('addi', 0): 2,\n",
       " ('goe', 0): 13,\n",
       " ('school', 0): 15,\n",
       " ('tue', 0): 3,\n",
       " ('wed', 0): 9,\n",
       " ('jesu', 0): 2,\n",
       " ('everybodi', 0): 5,\n",
       " ('impress', 0): 2,\n",
       " ('yesh', 0): 2,\n",
       " ('greatbhaji', 0): 2,\n",
       " ('kalli', 0): 5,\n",
       " ('cricket', 0): 2,\n",
       " ('sachin', 0): 2,\n",
       " ('worldveri', 0): 2,\n",
       " ('tv', 0): 15,\n",
       " ('youhow', 0): 1,\n",
       " ('perform', 0): 3,\n",
       " ('kappa', 0): 2,\n",
       " ('everywher', 0): 2,\n",
       " ('dirt', 0): 1,\n",
       " ('floor', 0): 3,\n",
       " ('window', 0): 4,\n",
       " ('sometim', 0): 5,\n",
       " ('mouth', 0): 1,\n",
       " ('flow', 0): 2,\n",
       " ('dream', 0): 22,\n",
       " ('world', 0): 23,\n",
       " ('chore', 0): 1,\n",
       " ('must', 0): 16,\n",
       " ('exist', 0): 1,\n",
       " ('hail', 0): 1,\n",
       " ('mist', 0): 1,\n",
       " ('slow', 0): 8,\n",
       " ('poo', 0): 1,\n",
       " ('angri', 0): 9,\n",
       " ('misbehav', 0): 2,\n",
       " ('plz', 0): 16,\n",
       " ('slap', 0): 2,\n",
       " ('urself', 0): 9,\n",
       " ('fault', 0): 5,\n",
       " ('speak', 0): 11,\n",
       " ('friendship', 0): 15,\n",
       " ('interest', 0): 8,\n",
       " ('english', 0): 3,\n",
       " ('muhommad', 0): 1,\n",
       " ('penni', 0): 1,\n",
       " ('across', 0): 4,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run this cell to get an idea about the corpus of words and their occurrence along with labels. \n",
    "## In this, we are computing the frequency of occurrence of word given that a message is 'spam'.\n",
    "## Similarly, we also compute the frequence of occurence of word given that a message is 'ham'.\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c24bc",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes Model: (20 points)\n",
    "\n",
    "Now we are in the training phase of the Naive Bayes algorithm. In this cell, take a look at the ways to calculate the log likelihood and log prior values as these are important for testing in the next few cells. \n",
    "\n",
    "Also calculate the frequency of occurrence of words where the output is spam. In the same way, calculate the word frequency count using the above functions in order to compute the log likelihood.\n",
    "\n",
    "Return the logprior and loglikelihood output by the model from this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f280e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of messages\n",
    "        train_y: a list of labels correponding to the messages (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set()\n",
    "    for key in freqs.keys():\n",
    "        vocab.add(key[0])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate num_pos and num_neg - the total number of positive and negative words for all documents\n",
    "    num_pos = num_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            num_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            num_neg += freqs[pair]\n",
    "\n",
    "    # Calculate num_doc, the number of documents\n",
    "    num_doc = len(train_x)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents \n",
    "    pos_num_docs = train_y.value_counts()[1]\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents \n",
    "    neg_num_docs = train_y.value_counts()[0]\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(pos_num_docs) - np.log(neg_num_docs)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = find_occurrence(freqs, word, 1)\n",
    "        freq_neg = find_occurrence(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1)/ (pos_num_docs + V)\n",
    "        p_w_neg = (freq_neg + 1)/ (neg_num_docs + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1561d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "6572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9c882",
   "metadata": {},
   "source": [
    "### Expected Output \n",
    "\n",
    "0.0 <br>\n",
    "6763"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b51303",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes Predict Function: (15 points)\n",
    "\n",
    "It is now time to make our prediction as to whether a given message is spam or ham respectively. \n",
    "\n",
    "After adding the log likelihood values, ensure that the output is 1 (spam) if the sum of the log likelihood value is greater than 0 and 0 (ham) if the sum of the log likelihood is less than or equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b692c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 CELL\n",
    "\n",
    "def naive_bayes_predict(message, logprior, loglikelihood):\n",
    "    '''\n",
    "    Params:\n",
    "        message: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Return:\n",
    "        total_prob: the sum of all the loglikelihoods of each word in the message (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "     # process the message to get a list of words\n",
    "    word_l = clean_message(message)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    total_prob = 0\n",
    "\n",
    "    # add the logprior\n",
    "    total_prob = logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood.keys():\n",
    "            # add the log likelihood of that word to the probability\n",
    "            total_prob += loglikelihood[word]\n",
    "\n",
    "\n",
    "    return 1 if total_prob > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b170333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1\n"
     ]
    }
   ],
   "source": [
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Experiment with your own message.\n",
    "my_message = 'here is an offer for you'\n",
    "p = naive_bayes_predict(my_message, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242708f",
   "metadata": {},
   "source": [
    "### Expected Output :\n",
    "The expected output is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4eeb71",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes Test function: (15 points)\n",
    "\n",
    "In this function, implement the previous functions such as naive_bayes_predict to get the predictions for the test set. \n",
    "\n",
    "In addition to this, the function should return the total number of messages that it correctly classified as 'spam' or 'ham'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66a511e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of messages\n",
    "        test_y: the corresponding labels for the list of messages\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of messages classified correctly)/(total # of message)\n",
    "    \"\"\"\n",
    "    accuracy = 0  \n",
    "\n",
    "    \n",
    "    y_hats = []\n",
    "    for message in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(message, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = sum(1 for expected, real in zip(test_y, y_hats) if expected != real) / len(test_y)\n",
    "\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# test_naive_bayes(X_test, y_test, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a9c5d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get an offer -> 1.00\n",
      "get the latest movie review -> 1.00\n",
      "order iphone -> 1.00\n",
      "I am on a meeting -> 0.00\n"
     ]
    }
   ],
   "source": [
    "# For grading purpose only\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Run this cell to test your function\n",
    "for message in ['get an offer', 'get the latest movie review', 'order iphone', 'I am on a meeting']:\n",
    "    # print( '%s -> %f' % (message, naive_bayes_predict(message, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(message, logprior, loglikelihood)\n",
    "#     print(f'{message} -> {p:.2f} ({p_category})')\n",
    "    print(f'{message} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2ef98",
   "metadata": {},
   "source": [
    "### Expected Output :\n",
    "get an offer -> 1.00 <br>\n",
    "get the latest movie review -> 1.00 <br>\n",
    "order iphone -> 1.00 <br>\n",
    "I am on a meeting -> 0.00 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "216fa97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own message below\n",
    "my_message = 'get an offer of iphone 13 and grab your order'\n",
    "naive_bayes_predict(my_message, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e4f0",
   "metadata": {},
   "source": [
    "### Expected Output :\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6e4d1",
   "metadata": {},
   "source": [
    "## Theory Questions: (15 points)\n",
    "\n",
    "1. When performing Naive Bayes operation especially for text classification, why is there a requirement for Laplace Smoothing or Additive Smoothing? Explain with considering an example of training and the test set and show how not having additive smoothing leads to undesirable outcomes. (10 points)  \n",
    "Answer: The Laplace or Additive Smoothing would prevent the value of \"probability of word\" being zero, which will further prevent the error when computing the log likelihood. Let's say the word \"language\" occurs only in the training dataset with negative labels instead of the training dataset with the positive labels. Here comes a document \"I like language processing.\", since the word \"language\" in the training dataset are all with negative labels, then the probability that \"language\" is positive is 0. In the end, the log likelihood of the \"language\" could not be computed since the log(0) does not exist.\n",
    "\n",
    "2. Why are logarithmic values computed for naive bayes algorithm rather than only the probability values? (5 points)  \n",
    "Answer: The log function is monotonically increasing, which has the same order relation as the original likelihood. Moreover, the usage of log function would transform the multiply of likelihood function to summation of the log likelihood function as what I did to the total_prob in the naive_bayes_predict() function, which would avoid the underflow when many decimals are multiplied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
