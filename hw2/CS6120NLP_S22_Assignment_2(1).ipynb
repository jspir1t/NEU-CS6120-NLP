{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNMrNlx_plWE"
      },
      "source": [
        "# **CS 6120: Natural Language Processing - Prof. Ahmad Uzair** \n",
        "\n",
        "### **Assignment 2: n-gram Language Models and Hierarchical Clustering **\n",
        "\n",
        "### **Total points: 100**\n",
        "\n",
        "In this assignment, You will be learning character level language models and hierarchical Clustering by implementing it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB23-0gxplWG"
      },
      "source": [
        "## <CENTER>PART-A "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9lthD13plWG"
      },
      "source": [
        "### OBJECTIVE : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM07ZgklplWG"
      },
      "source": [
        "Your task is to train n-gram language models. [Ref SLP Chapter 3]\n",
        "\n",
        "- Task 1: You will train unigram, bigram, and trigram models on given training files. Then you will score on given test files for unigram, bigram, and trigram. you will generate sentences from the trained model and compute perplexity.\n",
        "- Task 2: You will create training data for n > 3. and Repeat the above task from training model.\n",
        "<h6>Part-A = (55 Points) </h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "qpO0AKC8plWH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Your imports go here\n",
        "You are encouraged to implement your own functions and not use from library.\n",
        "'''\n",
        "import sys\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "iS3J6hisvReS"
      },
      "outputs": [],
      "source": [
        "# constants to define pseudo-word tokens\n",
        "# access via UNK, for instance\n",
        "# for this assignemnt we will follow <s> tag for beginning of sentence and\n",
        "# </s> for end of senetence as suggested in SLP Book. Check sample training files for reference.\n",
        "UNK = \"<UNK>\"\n",
        "SENT_BEGIN = \"<s>\"\n",
        "SENT_END = \"</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp00BMTJ4f8J"
      },
      "source": [
        "We need to initialise global variables for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "W_VUIkeUplWI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"Initializes Parameters:\n",
        "  n_gram (int): the n-gram order.\n",
        "  is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
        "  threshold: words with frequency  below threshold will be converted to token\n",
        "\"\"\"\n",
        "# Initializing different object attributes\n",
        "n_gram = 3\n",
        "is_laplace_smoothing = True\n",
        "vocab = [] \n",
        "n_gram_counts = {}\n",
        "n_minus_1_gram_counts = None\n",
        "threshold = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRYDPhLAplWI"
      },
      "source": [
        "### TASK - 1  = 20 points :\n",
        "Implement training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "X4L2wHUjvciI"
      },
      "outputs": [],
      "source": [
        "def make_ngrams(tokens: list, n: int) -> list:\n",
        "    \"\"\"Creates n-grams for the given token sequence.\n",
        "    Args:\n",
        "    tokens (list): a list of tokens as strings\n",
        "    n (int): the length of n-grams to create\n",
        "\n",
        "    Returns:\n",
        "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
        "    \"\"\"\n",
        "    n_grams = [tuple(tokens[i: i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    ## Your code here \n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "LHOwrc2Uvgtm"
      },
      "outputs": [],
      "source": [
        "def train(training_file_path):\n",
        "    \"\"\"Trains the language model on the given data. Input file that\n",
        "    has tokens that are white-space separated, has one sentence per line, and\n",
        "    that the sentences begin with <s> and end with </s>\n",
        "    Parameters:\n",
        "      training_file_path (str): the location of the training data to read\n",
        "\n",
        "    Returns:\n",
        "    N Gram Counts, Vocab, N Minus 1 Gram Counts\n",
        "    \"\"\"\n",
        "    with open(training_file_path, 'r') as fh:\n",
        "      content = fh.read().split() # Read and split data to get list of words\n",
        "    \n",
        "    # Get the count of each word\n",
        "    word_count = {}\n",
        "    for word in content:\n",
        "      # if word != SENT_BEGIN and word != SENT_END:\n",
        "      word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    # Replace the words with <UNK> if count is < threshold(=1)\n",
        "    content = [a if word_count.get(a, 0) >= threshold else UNK for a in content]\n",
        "    # make use of make_n_grams function\n",
        "    n_gram_tokens = make_ngrams(content, n_gram)\n",
        "    n_gram_counts = {}\n",
        "    for token in n_gram_tokens:\n",
        "      n_gram_counts[token] = n_gram_counts.get(token, 0) + 1\n",
        "    # Get the training data vocabulary\n",
        "    vocab = set(content)\n",
        "    # For n>1 grams compute n-1 gram counts to compute probability\n",
        "    if n_gram > 1:\n",
        "      n_minus_1_gram_counts = {}\n",
        "      n_minus_1_gram_tokens = make_ngrams(content, n_gram - 1)\n",
        "      for token in n_minus_1_gram_tokens:\n",
        "        n_minus_1_gram_counts[token] = n_minus_1_gram_counts.get(token, 0) + 1\n",
        "    return n_gram_counts, vocab, n_minus_1_gram_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ucWPcHIJly_"
      },
      "source": [
        "Output your Trained Data Parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7j8ct0BwocM",
        "outputId": "d9e230c4-63e9-4d9c-ab3a-85907b012701"
      },
      "outputs": [],
      "source": [
        "n_gram_counts, vocab, n_minus_1_gram_counts = train(\"train_data/berp-training-tri.txt\")\n",
        "print(n_gram_counts)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54hLufsd5lVZ"
      },
      "source": [
        "### TASK - 2  = 15 points :\n",
        "Implement Score function that will take input sentence and output probability of given string representing a single sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "qqJ05JFSvxIr"
      },
      "outputs": [],
      "source": [
        "def get_gram_log_prob(gram_counts, gram_token, total_value, V):\n",
        "  return np.log((gram_counts.get(gram_token, 0) + 1) / (total_value + V))\n",
        "\n",
        "def score(sentence):\n",
        "    \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
        "    Parameters:\n",
        "      sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
        "      \n",
        "    Returns:\n",
        "      float: the probability value of the given string for this model\n",
        "    \"\"\"\n",
        "    # Split the input sentence and replace out of vocabulary tokens with <UNK>     \n",
        "    content = sentence.split()\n",
        "    tokens = [a if a in vocab else UNK for a in content]\n",
        "    # Calculate probability for each word and multiply(or take log and sum) them to get the sentence probability\n",
        "    n_gram_tokens = make_ngrams(tokens, n_gram)\n",
        "    n_minus_1_gram_tokens = make_ngrams(tokens, n_gram - 1)\n",
        "    \n",
        "    # assert len(n_gram_tokens) == len(n_minus_1_gram_tokens) - 1\n",
        "\n",
        "    V = len(vocab)\n",
        "    n_gram_total_occurence = sum(n_gram_counts.values())\n",
        "    n_minus_1_gram_total_occurence = sum(n_minus_1_gram_counts.values())\n",
        "    # log_prob = get_gram_log_prob(n_minus_1_gram_counts, n_minus_1_gram_tokens[0], n_minus_1_gram_total_occurence, V)\n",
        "    log_prob = 0.\n",
        "    \n",
        "    for i in range(len(n_gram_tokens)):\n",
        "      log_prob += get_gram_log_prob(n_gram_counts, n_gram_tokens[i], n_gram_total_occurence, V) - get_gram_log_prob(n_minus_1_gram_counts, n_minus_1_gram_tokens[i+1], n_minus_1_gram_total_occurence, V)\n",
        "    return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "QaayxCgOzUUb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of test sentences:  100\n"
          ]
        }
      ],
      "source": [
        "with open(\"test_data/hw2-test-tri.txt\", 'r') as fh:\n",
        "    test_content = fh.read().split(\"\\n\")\n",
        "num_sentences_1 = len(test_content)\n",
        "ten_sentences_1 = test_content[:10]\n",
        "print(\"# of test sentences: \", num_sentences_1)\n",
        "probablities = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ei2Zrd61zeKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ -9.62919467  -9.08664378 -14.43860946 -10.26403105 -15.84579328\n",
            "  -9.40635558 -11.89418833 -13.67019696 -14.51702791  -8.81829239\n",
            "  -8.81827842  -9.9783271   -8.58901126  -6.27453127 -12.72893927\n",
            "  -6.83462795  -7.43199803  -7.6337237  -12.32101726 -11.95729738\n",
            " -24.72462736 -18.75078274 -17.5781891  -15.99296055 -11.98249451\n",
            " -11.2846587   -7.15850146 -10.04885925 -15.69625298 -14.12313717\n",
            "  -9.92045055 -19.64853378 -11.42118572 -13.51564497 -15.85570058\n",
            " -15.04749102  -8.25910777  -8.41091076 -11.36592056 -10.35326334\n",
            " -12.12451791 -11.05704468 -10.74928434 -13.57856054 -12.04615136\n",
            " -10.18135735  -9.04140801 -11.16963971 -12.92269157 -15.74217472\n",
            " -12.40171357 -12.19596036  -5.4906129   -4.03613369  -4.03613369\n",
            " -12.5023427  -12.64922474  -8.12514521  -6.33557972 -16.0424991\n",
            "  -3.44206078 -11.9106039  -17.440516    -8.41278538 -18.46283213\n",
            " -13.55822026  -8.81827842  -6.73885085  -8.12514521 -18.57026706\n",
            " -17.83387865 -15.47998286  -9.28665649  -9.17492544 -17.58147983\n",
            "  -8.81826446  -7.49259472 -13.53087346 -11.21614577 -14.29886149\n",
            " -10.17637398  -9.5114256   -8.81829239  -8.81826446 -13.65663003\n",
            "  -8.93604749 -25.86498197 -14.47033287  -5.08062277 -15.91395509\n",
            " -11.52630069  -9.2821724   -7.67039509 -12.55592011 -12.39917867\n",
            " -13.24080886  -9.00244296  -8.3505074  -17.09408166 -13.63911529]\n",
            "-11.699060067253658\n",
            "4.045742424138425\n"
          ]
        }
      ],
      "source": [
        "# print probabilities/score of sentences in test content\n",
        "for sentence in test_content:\n",
        "  probablities.append(score(sentence))\n",
        "probablities = np.array(probablities)\n",
        "mean = np.mean(probablities)\n",
        "std_dev = np.std(probablities)\n",
        "print(probablities)\n",
        "print(mean)\n",
        "print(std_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awEQeJtIzTrC"
      },
      "source": [
        "### TASK - 3  = 10 points :\n",
        "Generate sentence from the above trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "SpTKpxDMv2-4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence():\n",
        "  \"\"\"Generates a single sentence from a trained language model using the Shannon technique.\n",
        "    \n",
        "  Returns:\n",
        "    str: the generated sentence\n",
        "  \"\"\"\n",
        "  # Start with <s> and randomly generate words until we encounter sentence end\n",
        "  # Append sentence begin markers for n>2\n",
        "  # Keep track of previous word for stop condition\n",
        "  sentence = ['<s>']\n",
        "  prev_word = '<s>'\n",
        "  if n_gram > 1:\n",
        "    if n_gram > 2:\n",
        "      for _ in range(n_gram - 2):\n",
        "        sentence.append('<s>')\n",
        "    prev_token = ['<s>' for _ in range(n_gram-1)]\n",
        "    while prev_word != \"</s>\":\n",
        "      # Construct the (n-1) gram so far\n",
        "      # Get the counts of all available choices based on n-1 gram\n",
        "      # Convert the counts into probability for random.choice() function\n",
        "      # If <s> is generated, ignore and generate another word'\n",
        "      candidate_tokens = []\n",
        "      candidate_tokens_occurance = []\n",
        "      for token in n_gram_counts:\n",
        "        if list(token[:n_gram-1]) == prev_token:\n",
        "          candidate_tokens.append(token)\n",
        "          candidate_tokens_occurance.append(n_gram_counts[token])\n",
        "      picked_token = random.choices(candidate_tokens, candidate_tokens_occurance, k=1)[0]\n",
        "      picked_word = picked_token[-1]\n",
        "      if picked_word != '<s>':\n",
        "        sentence.append(picked_word)\n",
        "        prev_word = picked_word\n",
        "        prev_token = prev_token[1:]\n",
        "        prev_token.append(picked_word)\n",
        "\n",
        "  else:\n",
        "    # In case of unigram model, n-1 gram is just the previous word and possible choice is whole vocabulary\n",
        "    while prev_word != \"</s>\":\n",
        "      # Convert the counts into probability for random.choice() function\n",
        "      # If <s> is generated, ignore and generate another word\n",
        "      picked_word = random.choice(vocab)\n",
        "      if picked_word != '<s>':\n",
        "        sentence.append(picked_word)\n",
        "        prev_word = picked_word\n",
        "\n",
        "  # Append sentence end markers for n>2\n",
        "  if n_gram > 2:\n",
        "    for _ in range(n_gram - 2):\n",
        "      sentence.append('</s>')\n",
        "  return \" \".join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "aeQKR_UHv462"
      },
      "outputs": [],
      "source": [
        "def generate(n):\n",
        "    \"\"\"Generates n sentences from a trained language model using the Shannon technique.\n",
        "    Parameters:\n",
        "      n (int): the number of sentences to generate\n",
        "      \n",
        "    Returns:\n",
        "      list: a list containing strings, one per generated sentence\n",
        "    \"\"\"\n",
        "    # Generate sentences one by one and store\n",
        "    sentences = [generate_sentence() for _ in range(n)]\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "2UMoLPcvBKpn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences:\n",
            "<s> <s> doesn't matter </s> </s>\n",
            "<s> <s> i don't care if it's no chinese it will be going on thursday or friday or a weekend night </s> </s>\n",
            "<s> <s> i would like to go for breakfast </s> </s>\n",
            "<s> <s> i would like to go for lunch </s> </s>\n",
            "<s> <s> i'm looking for italian restaurant for brunch </s> </s>\n",
            "<s> <s> the distance is about half a mile </s> </s>\n",
            "<s> <s> after ten p__m </s> </s>\n",
            "<s> <s> uh find a japanese restaurant um casual for the evening would be good food i'm not very hungry i intend to spend uh between ten to fifteen dollar </s> </s>\n",
            "<s> <s> hamburger sounds nice </s> </s>\n",
            "<s> <s> start over please </s> </s>\n",
            "<s> <s> any day </s> </s>\n",
            "<s> <s> how about yangtze river </s> </s>\n",
            "<s> <s> i wanna keep it under fifteen dollars </s> </s>\n",
            "<s> <s> where do i find a really good greek restaurant something not as expensive </s> </s>\n",
            "<s> <s> tell me more information about soup kitchen heike </s> </s>\n",
            "<s> <s> expensive </s> </s>\n",
            "<s> <s> no more than thirty dollars </s> </s>\n",
            "<s> <s> can i have breakfast </s> </s>\n",
            "<s> <s> where can i go lunch it's any weekday if i wanna have some thai food for dinner </s> </s>\n",
            "<s> <s> pasand looks interesting </s> </s>\n",
            "<s> <s> less than five minutes </s> </s>\n",
            "<s> <s> i would like to know how to get a car available now so my friend to dinner </s> </s>\n",
            "<s> <s> start over </s> </s>\n",
            "<s> <s> the price but i don't care what the cost should be very close to icsi </s> </s>\n",
            "<s> <s> i want dinner on tuesday </s> </s>\n",
            "<s> <s> do you have any informations </s> </s>\n",
            "<s> <s> show me the restaurants that are not chinese food </s> </s>\n",
            "<s> <s> i'm looking for a mexican restaurant </s> </s>\n",
            "<s> <s> friday or saturday </s> </s>\n",
            "<s> <s> does this restaurant serve drinks </s> </s>\n",
            "<s> <s> show me the ones that are very far </s> </s>\n",
            "<s> <s> show me the santa-fe bar and coffee </s> </s>\n",
            "<s> <s> i would like to have uh good japanese restaurants </s> </s>\n",
            "<s> <s> what about nakapan </s> </s>\n",
            "<s> <s> take any of the week </s> </s>\n",
            "<s> <s> oh uh i'd like to go to dinner and um the cost doesn't really matter </s> </s>\n",
            "<s> <s> i'd like to know more about shilpa restaurant </s> </s>\n",
            "<s> <s> give me the list please </s> </s>\n",
            "<s> <s> the kind of drinks they have at shin-shin </s> </s>\n",
            "<s> <s> maybe french or an italian not a thai restaurant on martin luther king i forgot the name of it already </s> </s>\n",
            "<s> <s> i would like to spend a lot of time </s> </s>\n",
            "<s> <s> can you expand it to be somewhere near by that serves drinks </s> </s>\n",
            "<s> <s> a-la-carte addison annex </s> </s>\n",
            "<s> <s> lunch </s> </s>\n",
            "<s> <s> what do they serve there </s> </s>\n",
            "<s> <s> give me suggested restaurants </s> </s>\n",
            "<s> <s> i want to eat for twenty five dollars </s> </s>\n",
            "<s> <s> i want to eat dessert </s> </s>\n",
            "<s> <s> i said not japanese not indian </s> </s>\n",
            "<s> <s> i want to spend not very expensive </s> </s>\n"
          ]
        }
      ],
      "source": [
        "sentences = generate(50)\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-ok3UZEKmFi"
      },
      "source": [
        "### TASK - 4  = 5 points :\n",
        "Measures the perplexity for the test sequence with your trained model. \n",
        "you may assume that this sequence may consist of many sentences \"glued together\"\n",
        "\n",
        "The perplexity of the given sequence is the inverse probability of the test set, normalized by the number of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "G8mjYygFv7Gq"
      },
      "outputs": [],
      "source": [
        "# Since this sequence will cross many sentence boundaries, we need to include \n",
        "# the begin- and end-sentence markers <s> and </s> in the probability computation. \n",
        "# We also need to include the end-of-sentence marker </s> \n",
        "# but not the beginning-of-sentence marker <s>) in the total count of word tokens N\n",
        "\n",
        "def perplexity(test_sequence):\n",
        "    \"\"\".\n",
        "    Parameters:\n",
        "      test_sequence (string): a sequence of space-separated tokens to measure the perplexity of\n",
        "\n",
        "    Returns:\n",
        "      float: the perplexity of the given sequence\n",
        "    \"\"\" \n",
        "\n",
        "    # Replace out of vocab words with <UNK>, already done in score function\n",
        "    # test_sequence = [token if token in vocab else UNK for token in test_sequence.split()]\n",
        "    test_sequence = [token if token in vocab else UNK for token in test_sequence.split()]\n",
        "    n_gram_tokens = make_ngrams(test_sequence, n_gram)\n",
        "    n_minus_1_gram_tokens = make_ngrams(test_sequence, n_gram - 1)\n",
        "\n",
        "    V = len(vocab)\n",
        "    n_gram_total_occurence = sum(n_gram_counts.values())\n",
        "    n_minus_1_gram_total_occurence = sum(n_minus_1_gram_counts.values())\n",
        "    log_prob = 0.\n",
        "    for i in range(len(n_gram_tokens)):\n",
        "      log_prob += get_gram_log_prob(n_gram_counts, n_gram_tokens[i], n_gram_total_occurence, V) - get_gram_log_prob(n_minus_1_gram_counts, n_minus_1_gram_tokens[i+1], n_minus_1_gram_total_occurence, V)\n",
        "    log_prob = log_prob / float(len(n_gram_tokens))\n",
        "    perplexity = np.power(2, -log_prob)\n",
        "    # Remove sentence begin markers from data for computing N\n",
        "    # Get the probability for the sequence\n",
        "    \n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsldHtvIBlTV",
        "outputId": "981b0b5c-ff77-4777-c98b-12ffe0f9d529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0169398427882417\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(\" \".join(sentences[0:10])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfbHtUCW66Qk"
      },
      "source": [
        "### **Theory: (5 points)**\n",
        "* Experiment n_gram model for n = [1,2,3..7] of your choice. Explain the best choice of n that generates more meaninful sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrXwG40jplWN"
      },
      "source": [
        "# <CENTER> PART-B</CENTER> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdZLYTfkplWN"
      },
      "source": [
        "### OBJECTIVE : In this unsupervised learning task we are going to cluster wikipedia articles into groups using Hierarchical clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_SXd_VlCaWc"
      },
      "source": [
        "# **TASK-1 : 5 Points**\n",
        "##Download articles from Wikipedia\n",
        "In this section we will download articles from wikipedia and then cluster them into groups in the next step. You can select somewhat related topics or fetch the articles randomly. \n",
        "(Use dir() and help() functions or refer wikipedia documentation)\n",
        "You may also pick any other data source of your choice instead of wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05p2nS6agp0f"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "from wikipedia.exceptions import WikipediaException\n",
        "import pandas as pd\n",
        "\n",
        "'''\n",
        " Generate a list of wikipedia article titles to cluster \n",
        " You can maintain a static list of titles or generate them randomly using wikipedia library\n",
        " Some topics include:\n",
        " [\"Northeastern Unversity\", \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
        " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
        " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
        "\n",
        " You can add more topics from different categories so that we have a diverse datset to work with. \n",
        " Ex- About 10+ categories with 3+ article in each category\n",
        "'''\n",
        "# list of articles to be downloaded\n",
        "topics = [\"Northeastern Unversity\", \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
        " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
        " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
        "\n",
        "# download and store all the articles in this variable\n",
        "data=[]\n",
        "for topic in topics[1:2]:\n",
        "    titles = wikipedia.search(topic)\n",
        "    print(titles)\n",
        "    for title in titles:\n",
        "        try:\n",
        "            data.append(wikipedia.page(title).content)\n",
        "        except WikipediaException:\n",
        "            print(f\"error when searching for {topic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9LBbYwaX-Zf"
      },
      "source": [
        "# **TASK-2 : 5 Points**\n",
        "#Cleaning the Data\n",
        "In this step you will decide whether to clean the data or not. If you choose to clean, you may utilize the clean function from assignment 1\n",
        "\n",
        "**Answer(1-3 sentences):** Why are you (or not) choosing to clean the data? Think in terms of whether cleaning the data will help in the clustering or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp-atYsYSM2U"
      },
      "outputs": [],
      "source": [
        "# You can use Assignment 1's clean message function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSQJDf3ZaYzF"
      },
      "source": [
        "# **TASK-3 : 10 Points**\n",
        "##Vectorize the articles(5 points)\n",
        "In this step, we will vectorize the text data to use in hierarchical clustering. You can use countVectorizer() or TfidfVectorizer() from sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5N-b8AHg434"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK1Mc4h-qX_K"
      },
      "source": [
        "##Plot Dendogram (5 points)\n",
        "Now we will try to see the hierarchical relationship between articles using dendrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUlKRAtWiYKA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as shc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkEEe2z0ra5Q"
      },
      "source": [
        "After plotting the dendogram, you will see that if you cut the dendogram horizontally, you can seperate the data into groups. You will get different number of clusters depending on where you cut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJzEKl8sE6_"
      },
      "source": [
        "# **TASK-4 : 5 Points**\n",
        "#Apply Clustering\n",
        "In this step, we will assign cluster lables to each document/group using Agglomerative Hierarchical clustering.\n",
        "We can decide number of clusters based on the dendogram and our requirement (how many categories we want).(eg. n_clusters = 3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQlpRi_1kMue"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDMspHY2taxH"
      },
      "source": [
        "# **TASK-5 : 5 Points**\n",
        "#Word Clouds\n",
        "Now, we will try to visualize top 50 words in each cluster using word clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRCP9EI3keqF"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "for k in range(0, num_clusters):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uevRbAFKXbu_"
      },
      "source": [
        "Comment about the categorizion done by Hierarchical clustering. Are the groups meaningful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH3n203dnyPY"
      },
      "source": [
        "# **TASK-6 : 10 Points**\n",
        "# Apply Hierarchical clustering on spam dataset\n",
        "Now we will apply Hierarchical clustering(HC) on a subset(modify the fraction argument of the sample() function) of Assignment 1 data. We will try to see if Hierarchical clustering can perform good or not for a supervised problem. We will follow the same steps as above and apply HC on the message column of spam.csv and categorize them into two clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgbWW7N5krNu"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "# Read the data as done in Assignment 1\n",
        "## Reading the data and removing columns that are not important. \n",
        "## Renaming the columns so that we understand the columns easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh8D-KUdZRv-"
      },
      "source": [
        "#Task 7 Conclusion(5 points)\n",
        "Did Hierarchical clustering work as intended for spam classification? Why? \n",
        "(3-5 sentences)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS6120NLP_S22_Assignment_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
